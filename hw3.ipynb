{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.9.2",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.2 64-bit"
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "colab": {
      "name": "hw3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elio-li/CSCI4964/blob/main/hw3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-rf5hJXugIG"
      },
      "source": [
        "# Homework 3\n",
        "\n",
        "In your project, you will pick a dataset (not the same as in the previous homeworks) and describe the problem you would like to solve. Include a link to the dataset source. Next, you should pick a Deep Learning Framework that you would like to use to implement your 2-layer Neural Network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5juRVezugII"
      },
      "source": [
        "## Task 1 (25 points)\n",
        "\n",
        "Assuming you are not familiar with the framework, in this part of the homework you will present your research describing the resources you used to learn the framework (must include links to all resources). Clearly explain why you needed a particular resource for implementing a 2-layer Neural Network (NN). (Consider how you will keep track of all the computations in a NN i.e., what libraries/tools do you need within this framework.)\n",
        "\n",
        "For example, some of the known resources for TensorFlow are:\n",
        "- https://www.tensorflow.org/guide/autodiff\n",
        "- https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
        "\n",
        "Hint: You need to figure out the APIs/packages used to implement forward propagation and backward propagation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH8quABiy_yV"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gll28AVFugIJ"
      },
      "source": [
        "## Task 2 (60 points)\n",
        "\n",
        "Once you have figured the resources you need for the project, design, and implement your project. The project must include the following steps (itâ€™s not limited to these steps):\n",
        "1. Exploratory Data Analysis (Can include data cleaning, visualization etc.)\n",
        "2. Perform a train-dev-test split.\n",
        "3. Implement forward propagation (clearly describe the activation functions and other\n",
        "hyper-parameters you are using).\n",
        "4. Compute the final cost function.\n",
        "5. Implement mini-batch gradient descent to train your model. In this step it is up to you\n",
        "as someone in charge of their project to improvise using optimization algorithms\n",
        "(Adams, RMSProp etc.) and/or regularization.\n",
        "6. Present the results using the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypmOhRC5ugIK"
      },
      "source": [
        "# Dataset Link: https://archive-beta.ics.uci.edu/ml/datasets/spambase\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import make_column_transformer, make_column_selector\n",
        "\n",
        "# Read File, Get x & y\n",
        "df = pd.read_csv(\"spambase.csv\")\n",
        "y = df['label']\n",
        "x = df.drop(['label'], axis = 1)\n",
        "\n",
        "preprocessor = make_column_transformer(\n",
        "    (StandardScaler(),\n",
        "     make_column_selector(dtype_include=np.number)),\n",
        "    (OneHotEncoder(sparse=False),\n",
        "     make_column_selector(dtype_include=object)),\n",
        ")\n",
        "\n",
        "x = preprocessor.fit_transform(x)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKj-t3MXON2w",
        "outputId": "f52dffde-5377-4e0c-b6b0-6ffacab2137f"
      },
      "source": [
        "# 2. Perform a train-dev-test split.\n",
        "x_train, x_rem, y_train, y_rem = train_test_split(x, y, train_size=0.8)\n",
        "x_valid, x_test, y_valid, y_test = train_test_split(x_rem, y_rem, test_size=0.5)\n",
        "\n",
        "print('x_train:\\t{}'.format(x_train.shape))\n",
        "print('y_train:\\t{}'.format(y_train.shape))\n",
        "print('x_valid:\\t{}'.format(x_valid.shape))\n",
        "print('y_valid:\\t{}'.format(y_valid.shape))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train:\t(3680, 57)\n",
            "y_train:\t(3680,)\n",
            "x_valid:\t(460, 57)\n",
            "y_valid:\t(460,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aXK3UleaQ1y3",
        "outputId": "d907b1c0-f52f-41b0-c0cb-c7c6118f1ac3"
      },
      "source": [
        "# 3. Activation Funtion is 'RELU',\n",
        "#    For the 2-Layers NN, the hidden layer contains 70 neurons, where the output layer contains 1 neurons.\n",
        "#    Adam is used as the optimization algorithm and use mean square error as the regression loss.\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "  layers.Dense(units=50, activation='relu', input_shape=[57]),   \n",
        "  layers.Dense(units=1),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "  optimizer='adam',\n",
        "  loss='mean_squared_error'\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    validation_data=(x_valid, y_valid),\n",
        "    batch_size=50,\n",
        "    epochs=100\n",
        ")\n",
        "\n",
        "# convert the training history to a dataframe\n",
        "history_df = pd.DataFrame(history.history)\n",
        "# use Pandas native plot method\n",
        "history_df['loss'].plot();"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "74/74 [==============================] - 1s 4ms/step - loss: 0.4933 - val_loss: 0.2210\n",
            "Epoch 2/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.1932 - val_loss: 0.1480\n",
            "Epoch 3/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.1317 - val_loss: 0.1273\n",
            "Epoch 4/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.1070 - val_loss: 0.1127\n",
            "Epoch 5/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0918 - val_loss: 0.1048\n",
            "Epoch 6/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0827 - val_loss: 0.0987\n",
            "Epoch 7/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0757 - val_loss: 0.0930\n",
            "Epoch 8/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0706 - val_loss: 0.0917\n",
            "Epoch 9/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0669 - val_loss: 0.0863\n",
            "Epoch 10/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0630 - val_loss: 0.0831\n",
            "Epoch 11/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0607 - val_loss: 0.0804\n",
            "Epoch 12/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0582 - val_loss: 0.0773\n",
            "Epoch 13/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0565 - val_loss: 0.0773\n",
            "Epoch 14/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0554 - val_loss: 0.0749\n",
            "Epoch 15/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0527 - val_loss: 0.0743\n",
            "Epoch 16/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0523 - val_loss: 0.0716\n",
            "Epoch 17/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0512 - val_loss: 0.0736\n",
            "Epoch 18/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0510 - val_loss: 0.0696\n",
            "Epoch 19/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0494 - val_loss: 0.0690\n",
            "Epoch 20/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0493 - val_loss: 0.0675\n",
            "Epoch 21/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0492 - val_loss: 0.0677\n",
            "Epoch 22/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0515 - val_loss: 0.0663\n",
            "Epoch 23/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0480 - val_loss: 0.0699\n",
            "Epoch 24/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0472 - val_loss: 0.0660\n",
            "Epoch 25/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0457 - val_loss: 0.0642\n",
            "Epoch 26/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0446 - val_loss: 0.0631\n",
            "Epoch 27/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0441 - val_loss: 0.0645\n",
            "Epoch 28/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0424 - val_loss: 0.0639\n",
            "Epoch 29/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0424 - val_loss: 0.0619\n",
            "Epoch 30/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0414 - val_loss: 0.0616\n",
            "Epoch 31/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0409 - val_loss: 0.0622\n",
            "Epoch 32/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0416 - val_loss: 0.0620\n",
            "Epoch 33/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0403 - val_loss: 0.0633\n",
            "Epoch 34/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0407 - val_loss: 0.0614\n",
            "Epoch 35/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0416 - val_loss: 0.0637\n",
            "Epoch 36/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0424 - val_loss: 0.0623\n",
            "Epoch 37/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0428 - val_loss: 0.0645\n",
            "Epoch 38/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0456 - val_loss: 0.0612\n",
            "Epoch 39/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0402 - val_loss: 0.0597\n",
            "Epoch 40/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0389 - val_loss: 0.0587\n",
            "Epoch 41/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0390 - val_loss: 0.0602\n",
            "Epoch 42/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0373 - val_loss: 0.0588\n",
            "Epoch 43/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0367 - val_loss: 0.0602\n",
            "Epoch 44/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 0.0623\n",
            "Epoch 45/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0367 - val_loss: 0.0602\n",
            "Epoch 46/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0357 - val_loss: 0.0601\n",
            "Epoch 47/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 0.0612\n",
            "Epoch 48/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0364 - val_loss: 0.0588\n",
            "Epoch 49/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0367 - val_loss: 0.0626\n",
            "Epoch 50/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 0.0601\n",
            "Epoch 51/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0367 - val_loss: 0.0604\n",
            "Epoch 52/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0531 - val_loss: 0.0638\n",
            "Epoch 53/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0417 - val_loss: 0.0621\n",
            "Epoch 54/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0360 - val_loss: 0.0610\n",
            "Epoch 55/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 0.0608\n",
            "Epoch 56/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 0.0591\n",
            "Epoch 57/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 0.0604\n",
            "Epoch 58/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 0.0601\n",
            "Epoch 59/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0318 - val_loss: 0.0606\n",
            "Epoch 60/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0318 - val_loss: 0.0586\n",
            "Epoch 61/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 0.0593\n",
            "Epoch 62/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0639\n",
            "Epoch 63/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 0.0622\n",
            "Epoch 64/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 0.0630\n",
            "Epoch 65/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 0.0642\n",
            "Epoch 66/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0316 - val_loss: 0.0638\n",
            "Epoch 67/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0317 - val_loss: 0.0622\n",
            "Epoch 68/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0318 - val_loss: 0.0609\n",
            "Epoch 69/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0310 - val_loss: 0.0631\n",
            "Epoch 70/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 0.0634\n",
            "Epoch 71/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 0.0683\n",
            "Epoch 72/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 0.0618\n",
            "Epoch 73/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 0.0640\n",
            "Epoch 74/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0304 - val_loss: 0.0642\n",
            "Epoch 75/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0314 - val_loss: 0.0661\n",
            "Epoch 76/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0362 - val_loss: 0.0674\n",
            "Epoch 77/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0420 - val_loss: 0.0639\n",
            "Epoch 78/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 0.0636\n",
            "Epoch 79/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0304 - val_loss: 0.0632\n",
            "Epoch 80/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0652\n",
            "Epoch 81/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0647\n",
            "Epoch 82/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0636\n",
            "Epoch 83/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0656\n",
            "Epoch 84/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0676\n",
            "Epoch 85/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0693\n",
            "Epoch 86/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0670\n",
            "Epoch 87/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0678\n",
            "Epoch 88/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0662\n",
            "Epoch 89/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.0671\n",
            "Epoch 90/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.0674\n",
            "Epoch 91/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0317 - val_loss: 0.0691\n",
            "Epoch 92/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0642\n",
            "Epoch 93/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0316 - val_loss: 0.0683\n",
            "Epoch 94/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0358 - val_loss: 0.0663\n",
            "Epoch 95/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 0.0691\n",
            "Epoch 96/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0307 - val_loss: 0.0680\n",
            "Epoch 97/100\n",
            "74/74 [==============================] - 0s 3ms/step - loss: 0.0311 - val_loss: 0.0671\n",
            "Epoch 98/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0307 - val_loss: 0.0652\n",
            "Epoch 99/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0676\n",
            "Epoch 100/100\n",
            "74/74 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0709\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2tZuQX-pcyJ",
        "outputId": "6dfcd1b4-ad1c-48d4-b495-fc252b088e36"
      },
      "source": [
        "# 4. Evaluate with the test set\n",
        "model.evaluate(x=x_test, y=y_test)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 [==============================] - 0s 1ms/step - loss: 0.0601\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06014584004878998"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqA9BmapugIM"
      },
      "source": [
        "## Task 3 (15 points)\n",
        "\n",
        "In task 2 describe how you selected the hyperparameters. What was the rationale behind the technique you used? Did you use regularization? Why, or why not? Did you use an optimization algorithm? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1SHwqsMqn8o"
      },
      "source": [
        "Lets first see some results by manipulating the hyperparameters and optimization algorithms:\n",
        "\n",
        "| loss   | val_loss | neurons | activation | optimizer | batch_size | \n",
        "| ------ | -------- | ------- | ---------- | --------- | ---------- |\n",
        "| 0.0345 | 0.0628   | 70      | relu       | adam      | 100        |\n",
        "| 0.0451 | 0.0888   | 30      | relu       | adam      | 100        |\n",
        "| 0.0291 | 0.0616   | 110     | relu       | adam      | 100        |\n",
        "| 0.0308 | 0.0607   | 200     | relu       | adam      | 100        |\n",
        "\n",
        "We can see that less number of neurons leads to higher losses and validation losses, and greater number of neurons leads to lower loss. But we see no significant jump from 110 neurons to 200 neurons, also the cost function it generated tends to be unstable(lots of spikes on it). I believe this is caused by overfitting. Thus the best number of neurons should be around 100."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9yhaiFQ0Y4n"
      },
      "source": [
        "Now we will change the activation function to see the changes:\n",
        "\n",
        "| loss   | val_loss | neurons | activation | optimizer | batch_size | \n",
        "| ------ | -------- | ------- | ---------- | --------- | ---------- |\n",
        "| 0.0314 | 0.0631   | 100     | relu       | adam      | 100        |\n",
        "| 0.0547 | 0.0573   | 100     | sigmoid    | adam      | 100        |\n",
        "| 0.0376 | 0.0635   | 100     | tanh       | adam      | 100        |\n",
        "| 0.0363 | 0.0456   | 100     | softmax    | adam      | 100        |\n",
        "\n",
        "Overall, softmax activation function have both least losses, and sigmoid function have higher loss, but it's the best generalized, relu and tanh doesn't have too much differences, the difference between their loss and validation loss is the largest.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twb6zARD0bIN"
      },
      "source": [
        "Next we will change the optimizer to see what make it difference:\n",
        "\n",
        "| loss   | val_loss | neurons | activation | optimizer | batch_size | \n",
        "| ------ | -------- | ------- | ---------- | --------- | ---------- |\n",
        "| 0.0380 | 0.0455   | 100     | softmax    |           | 100        |\n",
        "| 0.0363 | 0.0456   | 100     | softmax    | adam      | 100        |\n",
        "| 0.0383 | 0.0456   | 100     | softmax    | RMSprop   | 100        |\n",
        "| 0.2222 | 0.2219   | 100     | softmax    | SGD       | 100        |\n",
        "| 0.3245 | 0.3234   | 100     | softmax    | Adagrad   | 100        |\n",
        "\n",
        "I'm a little confused by this, in the first row I did not use any optimizers but the result looks very similar to adam and RMSprop, while both SGD & Adagrad even make the result worse. In my opinion there are 2 possible explanations of this since optimizers shouldn't make my result worse, one explanation is something wrong with the dataset it self, another way explaning is that the model trained is already overfitting before applying any optimizers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWujNCL10dHZ"
      },
      "source": [
        "| loss   | val_loss | neurons | activation | optimizer | batch_size | \n",
        "| ------ | -------- | ------- | ---------- | --------- | ---------- |\n",
        "| 0.3245 | 0.3234   | 100     | softmax    | Adagrad   | 100        |\n",
        "| 0.1648 | 0.1749   | 50      | relu       | Adagrad   | 100        |\n",
        "| 0.1375 | 0.1443   | 50      | relu       | Adagrad   | 50         |\n",
        "| 0.1042 | 0.1102   | 70      | relu       | Adagrad   | 50         |\n",
        "\n",
        "After numbers of changing the parameters, I've successfully tuning the losses down to roughly 0.1, although this is still very far from my previous best result, this is still a big improvement for Adagrad optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6tj5YV61EDy"
      },
      "source": [
        "In general, I started the model training by using all the default parameters, and then I gradually adding, or changing the parameters one by one, and once I get some better result I will begin to play around all those parameters, finding the best set of values that can output the lowest losses."
      ]
    }
  ]
}